# VLM Model Configuration for Adversarial Patch Testing

# Primary model configuration
models:
  # Gemma-3 4B - Primary choice for balanced performance/memory
  gemma_4b:
    model_name: "google/gemma-2-9b-it"  # Using 9B as closest available
    model_type: "gemma"
    max_memory_gb: 12
    torch_dtype: "float16"  
    device_map: "auto"
    trust_remote_code: true
    cache_dir: "./models/cache"
    
  # Gemma-3 12B - Fallback for better performance if memory allows  
  gemma_12b:
    model_name: "google/gemma-2-27b-it"  # Using 27B as larger variant
    model_type: "gemma"
    max_memory_gb: 24
    torch_dtype: "float16"
    device_map: "auto" 
    trust_remote_code: true
    cache_dir: "./models/cache"

# Runtime configuration
runtime:
  # Memory management
  memory_threshold_gb: 16  # Max system memory to use
  check_memory_before_load: true
  enable_gradient_checkpointing: true
  
  # Inference settings
  max_new_tokens: 512
  temperature: 0.1
  do_sample: false
  pad_token_id: 0
  
  # Image processing
  image_size: [224, 224]
  normalize_images: true
  image_formats: [".jpg", ".jpeg", ".png", ".bmp"]

# Model selection priority (will try in order until successful)
model_priority:
  - "gemma_4b"
  - "gemma_12b" 

# System requirements
system:
  min_python_version: "3.8"
  min_torch_version: "2.1.0"
  min_transformers_version: "4.35.0"
  cuda_optional: true
  
# Logging configuration
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "logs/vlm_loader.log"