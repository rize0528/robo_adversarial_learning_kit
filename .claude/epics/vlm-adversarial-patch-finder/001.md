---
name: Environment & Model Setup
status: open
created: 2025-09-08T08:40:12Z
updated: 2025-09-08T08:40:12Z
github: [Will be updated when synced to GitHub]
depends_on: []
parallel: false
conflicts_with: []
---

# Task: Environment & Model Setup

## Description
Set up the development environment for adversarial patch generation against Vision-Language Models (VLMs). Install required dependencies, load the Gemma-3 4B VLM, and verify that the model can perform basic inference operations with sample images.

## Acceptance Criteria
- [ ] Python environment configured with all required dependencies
- [ ] PyTorch installed with CUDA support (if available)
- [ ] HuggingFace Transformers library installed and configured
- [ ] OpenCV installed for image processing
- [ ] Gemma-3 4B model successfully loaded via HuggingFace or Ollama
- [ ] Basic inference test completed with sample image
- [ ] Model outputs verified for correctness

## Technical Details
- Install PyTorch with appropriate CUDA version for the target hardware
- Set up HuggingFace Transformers for model loading and inference
- Configure OpenCV for image preprocessing operations
- Load Gemma-3 4B model using either HuggingFace Hub or local Ollama installation
- Implement basic inference pipeline to test model functionality
- Key files/locations: `requirements.txt`, `src/models/vlm_loader.py`, `tests/test_model_setup.py`

## Dependencies
- [ ] Python 3.8+ environment
- [ ] CUDA drivers (if using GPU)
- [ ] Sufficient disk space for model weights (~8GB for Gemma-3 4B)
- [ ] HuggingFace account and API token (if using HF Hub)

## Effort Estimate
- Size: M
- Hours: 8
- Parallel: false

## Definition of Done
- [ ] Code implemented
- [ ] Tests written and passing
- [ ] Documentation updated
- [ ] Code reviewed