# VLM Model Configuration for Adversarial Patch Testing

# Primary model configuration
models:
  # Gemma-3 4B - Primary choice for balanced performance/memory (multimodal VLM)
  gemma_4b:
    model_name: "google/gemma-3-4b-it"  # Actual Gemma 3 4B with vision capabilities
    model_type: "gemma3"
    max_memory_gb: 8  # Based on research: 6.4GB for BF16, allowing some buffer
    torch_dtype: "float16"  
    device_map: "auto"
    trust_remote_code: true
    cache_dir: "./models/cache"
    
  # Gemma-3 27B - Fallback for better performance if memory allows  
  gemma_27b:
    model_name: "google/gemma-3-27b-it"  # Actual Gemma 3 27B with vision capabilities
    model_type: "gemma3"
    max_memory_gb: 16  # Conservative estimate for 27B model
    torch_dtype: "float16"
    device_map: "auto" 
    trust_remote_code: true
    cache_dir: "./models/cache"

# Runtime configuration
runtime:
  # Memory management
  memory_threshold_gb: 16  # Max system memory to use
  check_memory_before_load: true
  enable_gradient_checkpointing: true
  
  # Device configuration
  device: "auto"  # Will auto-detect MPS on Apple Silicon, CUDA on NVIDIA GPUs, or CPU
  device_priority: ["mps", "cuda", "cpu"]  # Preferred device order
  
  # Apple Silicon optimizations
  mps:
    enabled: true
    fallback_to_cpu_on_error: true
    # MPS-specific memory optimizations
    empty_cache_after_inference: true
  
  # Inference settings
  max_new_tokens: 512
  temperature: 0.1
  do_sample: false
  pad_token_id: 0
  
  # Image processing
  image_size: [224, 224]
  normalize_images: true
  image_formats: [".jpg", ".jpeg", ".png", ".bmp"]

# Model selection priority (will try in order until successful)
model_priority:
  - "gemma_4b"
  - "gemma_27b" 

# System requirements
system:
  min_python_version: "3.8"
  min_torch_version: "2.1.0"
  min_transformers_version: "4.35.0"
  cuda_optional: true
  
# Logging configuration
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "logs/vlm_loader.log"